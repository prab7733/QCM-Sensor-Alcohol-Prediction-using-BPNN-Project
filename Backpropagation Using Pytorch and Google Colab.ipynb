{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pandas, scipy \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "            row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 14])\n",
      "torch.Size([25, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-47ebf1f30d55>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xPredicted = torch.tensor(X, dtype = torch.float)\n"
     ]
    }
   ],
   "source": [
    "# load and prepare data\n",
    "#xy = np.loadtxt('QCM Sensor Alcohol Dataset/QCM10.csv', delimiter=\";\", dtype=np.float32, skiprows=1)\n",
    "#X = xy[:,0:14]\n",
    "#y = xy[:, [0]]\n",
    "X = torch.tensor(X, dtype = torch.float)\n",
    "y = torch.tensor(y, dtype = torch.float)\n",
    "xPredicted = torch.tensor(X, dtype = torch.float)\n",
    "\n",
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_max, _ = torch.max(X, 0)\n",
    "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
    "\n",
    "X = torch.div(X, X_max)\n",
    "xPredicted_max = torch.div(xPredicted, xPredicted_max)\n",
    "y = y / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        # TODO: parameters can be parameterized instead of declaring them here\n",
    "        self.inputSize = 14\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 25\n",
    "        \n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 2 X 3 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "        \n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 1.259617805480957\n",
      "#1 Loss: 0.24852041900157928\n",
      "#2 Loss: 0.24852041900157928\n",
      "#3 Loss: 0.24852041900157928\n",
      "#4 Loss: 0.24852041900157928\n",
      "#5 Loss: 0.24852041900157928\n",
      "#6 Loss: 0.24852041900157928\n",
      "#7 Loss: 0.24852041900157928\n",
      "#8 Loss: 0.24852041900157928\n",
      "#9 Loss: 0.24852041900157928\n",
      "#10 Loss: 0.24852041900157928\n",
      "#11 Loss: 0.24852041900157928\n",
      "#12 Loss: 0.24852041900157928\n",
      "#13 Loss: 0.24852041900157928\n",
      "#14 Loss: 0.24852041900157928\n",
      "#15 Loss: 0.24852041900157928\n",
      "#16 Loss: 0.24852041900157928\n",
      "#17 Loss: 0.24852041900157928\n",
      "#18 Loss: 0.24852041900157928\n",
      "#19 Loss: 0.24852041900157928\n",
      "#20 Loss: 0.24852041900157928\n",
      "#21 Loss: 0.24852041900157928\n",
      "#22 Loss: 0.24852041900157928\n",
      "#23 Loss: 0.24852041900157928\n",
      "#24 Loss: 0.24852041900157928\n",
      "#25 Loss: 0.24852041900157928\n",
      "#26 Loss: 0.24852041900157928\n",
      "#27 Loss: 0.24852041900157928\n",
      "#28 Loss: 0.24852041900157928\n",
      "#29 Loss: 0.24852041900157928\n",
      "#30 Loss: 0.24852041900157928\n",
      "#31 Loss: 0.24852041900157928\n",
      "#32 Loss: 0.24852041900157928\n",
      "#33 Loss: 0.24852041900157928\n",
      "#34 Loss: 0.24852041900157928\n",
      "#35 Loss: 0.24852041900157928\n",
      "#36 Loss: 0.24852041900157928\n",
      "#37 Loss: 0.24852041900157928\n",
      "#38 Loss: 0.24852041900157928\n",
      "#39 Loss: 0.24852041900157928\n",
      "#40 Loss: 0.24852041900157928\n",
      "#41 Loss: 0.24852041900157928\n",
      "#42 Loss: 0.24852041900157928\n",
      "#43 Loss: 0.24852041900157928\n",
      "#44 Loss: 0.24852041900157928\n",
      "#45 Loss: 0.24852041900157928\n",
      "#46 Loss: 0.24852041900157928\n",
      "#47 Loss: 0.24852041900157928\n",
      "#48 Loss: 0.24852041900157928\n",
      "#49 Loss: 0.24852041900157928\n",
      "#50 Loss: 0.24852041900157928\n",
      "#51 Loss: 0.24852041900157928\n",
      "#52 Loss: 0.24852041900157928\n",
      "#53 Loss: 0.24852041900157928\n",
      "#54 Loss: 0.24852041900157928\n",
      "#55 Loss: 0.24852041900157928\n",
      "#56 Loss: 0.24852041900157928\n",
      "#57 Loss: 0.24852041900157928\n",
      "#58 Loss: 0.24852041900157928\n",
      "#59 Loss: 0.24852041900157928\n",
      "#60 Loss: 0.24852041900157928\n",
      "#61 Loss: 0.24852041900157928\n",
      "#62 Loss: 0.24852041900157928\n",
      "#63 Loss: 0.24852041900157928\n",
      "#64 Loss: 0.24852041900157928\n",
      "#65 Loss: 0.24852041900157928\n",
      "#66 Loss: 0.24852041900157928\n",
      "#67 Loss: 0.24852041900157928\n",
      "#68 Loss: 0.24852041900157928\n",
      "#69 Loss: 0.24852041900157928\n",
      "#70 Loss: 0.24852041900157928\n",
      "#71 Loss: 0.24852041900157928\n",
      "#72 Loss: 0.24852041900157928\n",
      "#73 Loss: 0.24852041900157928\n",
      "#74 Loss: 0.24852041900157928\n",
      "#75 Loss: 0.24852041900157928\n",
      "#76 Loss: 0.24852041900157928\n",
      "#77 Loss: 0.24852041900157928\n",
      "#78 Loss: 0.24852041900157928\n",
      "#79 Loss: 0.24852041900157928\n",
      "#80 Loss: 0.24852041900157928\n",
      "#81 Loss: 0.24852041900157928\n",
      "#82 Loss: 0.24852041900157928\n",
      "#83 Loss: 0.24852041900157928\n",
      "#84 Loss: 0.24852041900157928\n",
      "#85 Loss: 0.24852041900157928\n",
      "#86 Loss: 0.24852041900157928\n",
      "#87 Loss: 0.24852041900157928\n",
      "#88 Loss: 0.24852041900157928\n",
      "#89 Loss: 0.24852041900157928\n",
      "#90 Loss: 0.24852041900157928\n",
      "#91 Loss: 0.24852041900157928\n",
      "#92 Loss: 0.24852041900157928\n",
      "#93 Loss: 0.24852041900157928\n",
      "#94 Loss: 0.24852041900157928\n",
      "#95 Loss: 0.24852041900157928\n",
      "#96 Loss: 0.24852041900157928\n",
      "#97 Loss: 0.24852041900157928\n",
      "#98 Loss: 0.24852041900157928\n",
      "#99 Loss: 0.24852041900157928\n",
      "#100 Loss: 0.24852041900157928\n",
      "#101 Loss: 0.24852041900157928\n",
      "#102 Loss: 0.24852041900157928\n",
      "#103 Loss: 0.24852041900157928\n",
      "#104 Loss: 0.24852041900157928\n",
      "#105 Loss: 0.24852041900157928\n",
      "#106 Loss: 0.24852041900157928\n",
      "#107 Loss: 0.24852041900157928\n",
      "#108 Loss: 0.24852041900157928\n",
      "#109 Loss: 0.24852041900157928\n",
      "#110 Loss: 0.24852041900157928\n",
      "#111 Loss: 0.24852041900157928\n",
      "#112 Loss: 0.24852041900157928\n",
      "#113 Loss: 0.24852041900157928\n",
      "#114 Loss: 0.24852041900157928\n",
      "#115 Loss: 0.24852041900157928\n",
      "#116 Loss: 0.24852041900157928\n",
      "#117 Loss: 0.24852041900157928\n",
      "#118 Loss: 0.24852041900157928\n",
      "#119 Loss: 0.24852041900157928\n",
      "#120 Loss: 0.24852041900157928\n",
      "#121 Loss: 0.24852041900157928\n",
      "#122 Loss: 0.24852041900157928\n",
      "#123 Loss: 0.24852041900157928\n",
      "#124 Loss: 0.24852041900157928\n",
      "#125 Loss: 0.24852041900157928\n",
      "#126 Loss: 0.24852041900157928\n",
      "#127 Loss: 0.24852041900157928\n",
      "#128 Loss: 0.24852041900157928\n",
      "#129 Loss: 0.24852041900157928\n",
      "#130 Loss: 0.24852041900157928\n",
      "#131 Loss: 0.24852041900157928\n",
      "#132 Loss: 0.24852041900157928\n",
      "#133 Loss: 0.24852041900157928\n",
      "#134 Loss: 0.24852041900157928\n",
      "#135 Loss: 0.24852041900157928\n",
      "#136 Loss: 0.24852041900157928\n",
      "#137 Loss: 0.24852041900157928\n",
      "#138 Loss: 0.24852041900157928\n",
      "#139 Loss: 0.24852041900157928\n",
      "#140 Loss: 0.24852041900157928\n",
      "#141 Loss: 0.24852041900157928\n",
      "#142 Loss: 0.24852041900157928\n",
      "#143 Loss: 0.24852041900157928\n",
      "#144 Loss: 0.24852041900157928\n",
      "#145 Loss: 0.24852041900157928\n",
      "#146 Loss: 0.24852041900157928\n",
      "#147 Loss: 0.24852041900157928\n",
      "#148 Loss: 0.24852041900157928\n",
      "#149 Loss: 0.24852041900157928\n",
      "#150 Loss: 0.24852041900157928\n",
      "#151 Loss: 0.24852041900157928\n",
      "#152 Loss: 0.24852041900157928\n",
      "#153 Loss: 0.24852041900157928\n",
      "#154 Loss: 0.24852041900157928\n",
      "#155 Loss: 0.24852041900157928\n",
      "#156 Loss: 0.24852041900157928\n",
      "#157 Loss: 0.24852041900157928\n",
      "#158 Loss: 0.24852041900157928\n",
      "#159 Loss: 0.24852041900157928\n",
      "#160 Loss: 0.24852041900157928\n",
      "#161 Loss: 0.24852041900157928\n",
      "#162 Loss: 0.24852041900157928\n",
      "#163 Loss: 0.24852041900157928\n",
      "#164 Loss: 0.24852041900157928\n",
      "#165 Loss: 0.24852041900157928\n",
      "#166 Loss: 0.24852041900157928\n",
      "#167 Loss: 0.24852041900157928\n",
      "#168 Loss: 0.24852041900157928\n",
      "#169 Loss: 0.24852041900157928\n",
      "#170 Loss: 0.24852041900157928\n",
      "#171 Loss: 0.24852041900157928\n",
      "#172 Loss: 0.24852041900157928\n",
      "#173 Loss: 0.24852041900157928\n",
      "#174 Loss: 0.24852041900157928\n",
      "#175 Loss: 0.24852041900157928\n",
      "#176 Loss: 0.24852041900157928\n",
      "#177 Loss: 0.24852041900157928\n",
      "#178 Loss: 0.24852041900157928\n",
      "#179 Loss: 0.24852041900157928\n",
      "#180 Loss: 0.24852041900157928\n",
      "#181 Loss: 0.24852041900157928\n",
      "#182 Loss: 0.24852041900157928\n",
      "#183 Loss: 0.24852041900157928\n",
      "#184 Loss: 0.24852041900157928\n",
      "#185 Loss: 0.24852041900157928\n",
      "#186 Loss: 0.24852041900157928\n",
      "#187 Loss: 0.24852041900157928\n",
      "#188 Loss: 0.24852041900157928\n",
      "#189 Loss: 0.24852041900157928\n",
      "#190 Loss: 0.24852041900157928\n",
      "#191 Loss: 0.24852041900157928\n",
      "#192 Loss: 0.24852041900157928\n",
      "#193 Loss: 0.24852041900157928\n",
      "#194 Loss: 0.24852041900157928\n",
      "#195 Loss: 0.24852041900157928\n",
      "#196 Loss: 0.24852041900157928\n",
      "#197 Loss: 0.24852041900157928\n",
      "#198 Loss: 0.24852041900157928\n",
      "#199 Loss: 0.24852041900157928\n",
      "#200 Loss: 0.24852041900157928\n",
      "#201 Loss: 0.24852041900157928\n",
      "#202 Loss: 0.24852041900157928\n",
      "#203 Loss: 0.24852041900157928\n",
      "#204 Loss: 0.24852041900157928\n",
      "#205 Loss: 0.24852041900157928\n",
      "#206 Loss: 0.24852041900157928\n",
      "#207 Loss: 0.24852041900157928\n",
      "#208 Loss: 0.24852041900157928\n",
      "#209 Loss: 0.24852041900157928\n",
      "#210 Loss: 0.24852041900157928\n",
      "#211 Loss: 0.24852041900157928\n",
      "#212 Loss: 0.24852041900157928\n",
      "#213 Loss: 0.24852041900157928\n",
      "#214 Loss: 0.24852041900157928\n",
      "#215 Loss: 0.24852041900157928\n",
      "#216 Loss: 0.24852041900157928\n",
      "#217 Loss: 0.24852041900157928\n",
      "#218 Loss: 0.24852041900157928\n",
      "#219 Loss: 0.24852041900157928\n",
      "#220 Loss: 0.24852041900157928\n",
      "#221 Loss: 0.24852041900157928\n",
      "#222 Loss: 0.24852041900157928\n",
      "#223 Loss: 0.24852041900157928\n",
      "#224 Loss: 0.24852041900157928\n",
      "#225 Loss: 0.24852041900157928\n",
      "#226 Loss: 0.24852041900157928\n",
      "#227 Loss: 0.24852041900157928\n",
      "#228 Loss: 0.24852041900157928\n",
      "#229 Loss: 0.24852041900157928\n",
      "#230 Loss: 0.24852041900157928\n",
      "#231 Loss: 0.24852041900157928\n",
      "#232 Loss: 0.24852041900157928\n",
      "#233 Loss: 0.24852041900157928\n",
      "#234 Loss: 0.24852041900157928\n",
      "#235 Loss: 0.24852041900157928\n",
      "#236 Loss: 0.24852041900157928\n",
      "#237 Loss: 0.24852041900157928\n",
      "#238 Loss: 0.24852041900157928\n",
      "#239 Loss: 0.24852041900157928\n",
      "#240 Loss: 0.24852041900157928\n",
      "#241 Loss: 0.24852041900157928\n",
      "#242 Loss: 0.24852041900157928\n",
      "#243 Loss: 0.24852041900157928\n",
      "#244 Loss: 0.24852041900157928\n",
      "#245 Loss: 0.24852041900157928\n",
      "#246 Loss: 0.24852041900157928\n",
      "#247 Loss: 0.24852041900157928\n",
      "#248 Loss: 0.24852041900157928\n",
      "#249 Loss: 0.24852041900157928\n",
      "#250 Loss: 0.24852041900157928\n",
      "#251 Loss: 0.24852041900157928\n",
      "#252 Loss: 0.24852041900157928\n",
      "#253 Loss: 0.24852041900157928\n",
      "#254 Loss: 0.24852041900157928\n",
      "#255 Loss: 0.24852041900157928\n",
      "#256 Loss: 0.24852041900157928\n",
      "#257 Loss: 0.24852041900157928\n",
      "#258 Loss: 0.24852041900157928\n",
      "#259 Loss: 0.24852041900157928\n",
      "#260 Loss: 0.24852041900157928\n",
      "#261 Loss: 0.24852041900157928\n",
      "#262 Loss: 0.24852041900157928\n",
      "#263 Loss: 0.24852041900157928\n",
      "#264 Loss: 0.24852041900157928\n",
      "#265 Loss: 0.24852041900157928\n",
      "#266 Loss: 0.24852041900157928\n",
      "#267 Loss: 0.24852041900157928\n",
      "#268 Loss: 0.24852041900157928\n",
      "#269 Loss: 0.24852041900157928\n",
      "#270 Loss: 0.24852041900157928\n",
      "#271 Loss: 0.24852041900157928\n",
      "#272 Loss: 0.24852041900157928\n",
      "#273 Loss: 0.24852041900157928\n",
      "#274 Loss: 0.24852041900157928\n",
      "#275 Loss: 0.24852041900157928\n",
      "#276 Loss: 0.24852041900157928\n",
      "#277 Loss: 0.24852041900157928\n",
      "#278 Loss: 0.24852041900157928\n",
      "#279 Loss: 0.24852041900157928\n",
      "#280 Loss: 0.24852041900157928\n",
      "#281 Loss: 0.24852041900157928\n",
      "#282 Loss: 0.24852041900157928\n",
      "#283 Loss: 0.24852041900157928\n",
      "#284 Loss: 0.24852041900157928\n",
      "#285 Loss: 0.24852041900157928\n",
      "#286 Loss: 0.24852041900157928\n",
      "#287 Loss: 0.24852041900157928\n",
      "#288 Loss: 0.24852041900157928\n",
      "#289 Loss: 0.24852041900157928\n",
      "#290 Loss: 0.24852041900157928\n",
      "#291 Loss: 0.24852041900157928\n",
      "#292 Loss: 0.24852041900157928\n",
      "#293 Loss: 0.24852041900157928\n",
      "#294 Loss: 0.24852041900157928\n",
      "#295 Loss: 0.24852041900157928\n",
      "#296 Loss: 0.24852041900157928\n",
      "#297 Loss: 0.24852041900157928\n",
      "#298 Loss: 0.24852041900157928\n",
      "#299 Loss: 0.24852041900157928\n",
      "#300 Loss: 0.24852041900157928\n",
      "#301 Loss: 0.24852041900157928\n",
      "#302 Loss: 0.24852041900157928\n",
      "#303 Loss: 0.24852041900157928\n",
      "#304 Loss: 0.24852041900157928\n",
      "#305 Loss: 0.24852041900157928\n",
      "#306 Loss: 0.24852041900157928\n",
      "#307 Loss: 0.24852041900157928\n",
      "#308 Loss: 0.24852041900157928\n",
      "#309 Loss: 0.24852041900157928\n",
      "#310 Loss: 0.24852041900157928\n",
      "#311 Loss: 0.24852041900157928\n",
      "#312 Loss: 0.24852041900157928\n",
      "#313 Loss: 0.24852041900157928\n",
      "#314 Loss: 0.24852041900157928\n",
      "#315 Loss: 0.24852041900157928\n",
      "#316 Loss: 0.24852041900157928\n",
      "#317 Loss: 0.24852041900157928\n",
      "#318 Loss: 0.24852041900157928\n",
      "#319 Loss: 0.24852041900157928\n",
      "#320 Loss: 0.24852041900157928\n",
      "#321 Loss: 0.24852041900157928\n",
      "#322 Loss: 0.24852041900157928\n",
      "#323 Loss: 0.24852041900157928\n",
      "#324 Loss: 0.24852041900157928\n",
      "#325 Loss: 0.24852041900157928\n",
      "#326 Loss: 0.24852041900157928\n",
      "#327 Loss: 0.24852041900157928\n",
      "#328 Loss: 0.24852041900157928\n",
      "#329 Loss: 0.24852041900157928\n",
      "#330 Loss: 0.24852041900157928\n",
      "#331 Loss: 0.24852041900157928\n",
      "#332 Loss: 0.24852041900157928\n",
      "#333 Loss: 0.24852041900157928\n",
      "#334 Loss: 0.24852041900157928\n",
      "#335 Loss: 0.24852041900157928\n",
      "#336 Loss: 0.24852041900157928\n",
      "#337 Loss: 0.24852041900157928\n",
      "#338 Loss: 0.24852041900157928\n",
      "#339 Loss: 0.24852041900157928\n",
      "#340 Loss: 0.24852041900157928\n",
      "#341 Loss: 0.24852041900157928\n",
      "#342 Loss: 0.24852041900157928\n",
      "#343 Loss: 0.24852041900157928\n",
      "#344 Loss: 0.24852041900157928\n",
      "#345 Loss: 0.24852041900157928\n",
      "#346 Loss: 0.24852041900157928\n",
      "#347 Loss: 0.24852041900157928\n",
      "#348 Loss: 0.24852041900157928\n",
      "#349 Loss: 0.24852041900157928\n",
      "#350 Loss: 0.24852041900157928\n",
      "#351 Loss: 0.24852041900157928\n",
      "#352 Loss: 0.24852041900157928\n",
      "#353 Loss: 0.24852041900157928\n",
      "#354 Loss: 0.24852041900157928\n",
      "#355 Loss: 0.24852041900157928\n",
      "#356 Loss: 0.24852041900157928\n",
      "#357 Loss: 0.24852041900157928\n",
      "#358 Loss: 0.24852041900157928\n",
      "#359 Loss: 0.24852041900157928\n",
      "#360 Loss: 0.24852041900157928\n",
      "#361 Loss: 0.24852041900157928\n",
      "#362 Loss: 0.24852041900157928\n",
      "#363 Loss: 0.24852041900157928\n",
      "#364 Loss: 0.24852041900157928\n",
      "#365 Loss: 0.24852041900157928\n",
      "#366 Loss: 0.24852041900157928\n",
      "#367 Loss: 0.24852041900157928\n",
      "#368 Loss: 0.24852041900157928\n",
      "#369 Loss: 0.24852041900157928\n",
      "#370 Loss: 0.24852041900157928\n",
      "#371 Loss: 0.24852041900157928\n",
      "#372 Loss: 0.24852041900157928\n",
      "#373 Loss: 0.24852041900157928\n",
      "#374 Loss: 0.24852041900157928\n",
      "#375 Loss: 0.24852041900157928\n",
      "#376 Loss: 0.24852041900157928\n",
      "#377 Loss: 0.24852041900157928\n",
      "#378 Loss: 0.24852041900157928\n",
      "#379 Loss: 0.24852041900157928\n",
      "#380 Loss: 0.24852041900157928\n",
      "#381 Loss: 0.24852041900157928\n",
      "#382 Loss: 0.24852041900157928\n",
      "#383 Loss: 0.24852041900157928\n",
      "#384 Loss: 0.24852041900157928\n",
      "#385 Loss: 0.24852041900157928\n",
      "#386 Loss: 0.24852041900157928\n",
      "#387 Loss: 0.24852041900157928\n",
      "#388 Loss: 0.24852041900157928\n",
      "#389 Loss: 0.24852041900157928\n",
      "#390 Loss: 0.24852041900157928\n",
      "#391 Loss: 0.24852041900157928\n",
      "#392 Loss: 0.24852041900157928\n",
      "#393 Loss: 0.24852041900157928\n",
      "#394 Loss: 0.24852041900157928\n",
      "#395 Loss: 0.24852041900157928\n",
      "#396 Loss: 0.24852041900157928\n",
      "#397 Loss: 0.24852041900157928\n",
      "#398 Loss: 0.24852041900157928\n",
      "#399 Loss: 0.24852041900157928\n",
      "#400 Loss: 0.24852041900157928\n",
      "#401 Loss: 0.24852041900157928\n",
      "#402 Loss: 0.24852041900157928\n",
      "#403 Loss: 0.24852041900157928\n",
      "#404 Loss: 0.24852041900157928\n",
      "#405 Loss: 0.24852041900157928\n",
      "#406 Loss: 0.24852041900157928\n",
      "#407 Loss: 0.24852041900157928\n",
      "#408 Loss: 0.24852041900157928\n",
      "#409 Loss: 0.24852041900157928\n",
      "#410 Loss: 0.24852041900157928\n",
      "#411 Loss: 0.24852041900157928\n",
      "#412 Loss: 0.24852041900157928\n",
      "#413 Loss: 0.24852041900157928\n",
      "#414 Loss: 0.24852041900157928\n",
      "#415 Loss: 0.24852041900157928\n",
      "#416 Loss: 0.24852041900157928\n",
      "#417 Loss: 0.24852041900157928\n",
      "#418 Loss: 0.24852041900157928\n",
      "#419 Loss: 0.24852041900157928\n",
      "#420 Loss: 0.24852041900157928\n",
      "#421 Loss: 0.24852041900157928\n",
      "#422 Loss: 0.24852041900157928\n",
      "#423 Loss: 0.24852041900157928\n",
      "#424 Loss: 0.24852041900157928\n",
      "#425 Loss: 0.24852041900157928\n",
      "#426 Loss: 0.24852041900157928\n",
      "#427 Loss: 0.24852041900157928\n",
      "#428 Loss: 0.24852041900157928\n",
      "#429 Loss: 0.24852041900157928\n",
      "#430 Loss: 0.24852041900157928\n",
      "#431 Loss: 0.24852041900157928\n",
      "#432 Loss: 0.24852041900157928\n",
      "#433 Loss: 0.24852041900157928\n",
      "#434 Loss: 0.24852041900157928\n",
      "#435 Loss: 0.24852041900157928\n",
      "#436 Loss: 0.24852041900157928\n",
      "#437 Loss: 0.24852041900157928\n",
      "#438 Loss: 0.24852041900157928\n",
      "#439 Loss: 0.24852041900157928\n",
      "#440 Loss: 0.24852041900157928\n",
      "#441 Loss: 0.24852041900157928\n",
      "#442 Loss: 0.24852041900157928\n",
      "#443 Loss: 0.24852041900157928\n",
      "#444 Loss: 0.24852041900157928\n",
      "#445 Loss: 0.24852041900157928\n",
      "#446 Loss: 0.24852041900157928\n",
      "#447 Loss: 0.24852041900157928\n",
      "#448 Loss: 0.24852041900157928\n",
      "#449 Loss: 0.24852041900157928\n",
      "#450 Loss: 0.24852041900157928\n",
      "#451 Loss: 0.24852041900157928\n",
      "#452 Loss: 0.24852041900157928\n",
      "#453 Loss: 0.24852041900157928\n",
      "#454 Loss: 0.24852041900157928\n",
      "#455 Loss: 0.24852041900157928\n",
      "#456 Loss: 0.24852041900157928\n",
      "#457 Loss: 0.24852041900157928\n",
      "#458 Loss: 0.24852041900157928\n",
      "#459 Loss: 0.24852041900157928\n",
      "#460 Loss: 0.24852041900157928\n",
      "#461 Loss: 0.24852041900157928\n",
      "#462 Loss: 0.24852041900157928\n",
      "#463 Loss: 0.24852041900157928\n",
      "#464 Loss: 0.24852041900157928\n",
      "#465 Loss: 0.24852041900157928\n",
      "#466 Loss: 0.24852041900157928\n",
      "#467 Loss: 0.24852041900157928\n",
      "#468 Loss: 0.24852041900157928\n",
      "#469 Loss: 0.24852041900157928\n",
      "#470 Loss: 0.24852041900157928\n",
      "#471 Loss: 0.24852041900157928\n",
      "#472 Loss: 0.24852041900157928\n",
      "#473 Loss: 0.24852041900157928\n",
      "#474 Loss: 0.24852041900157928\n",
      "#475 Loss: 0.24852041900157928\n",
      "#476 Loss: 0.24852041900157928\n",
      "#477 Loss: 0.24852041900157928\n",
      "#478 Loss: 0.24852041900157928\n",
      "#479 Loss: 0.24852041900157928\n",
      "#480 Loss: 0.24852041900157928\n",
      "#481 Loss: 0.24852041900157928\n",
      "#482 Loss: 0.24852041900157928\n",
      "#483 Loss: 0.24852041900157928\n",
      "#484 Loss: 0.24852041900157928\n",
      "#485 Loss: 0.24852041900157928\n",
      "#486 Loss: 0.24852041900157928\n",
      "#487 Loss: 0.24852041900157928\n",
      "#488 Loss: 0.24852041900157928\n",
      "#489 Loss: 0.24852041900157928\n",
      "#490 Loss: 0.24852041900157928\n",
      "#491 Loss: 0.24852041900157928\n",
      "#492 Loss: 0.24852041900157928\n",
      "#493 Loss: 0.24852041900157928\n",
      "#494 Loss: 0.24852041900157928\n",
      "#495 Loss: 0.24852041900157928\n",
      "#496 Loss: 0.24852041900157928\n",
      "#497 Loss: 0.24852041900157928\n",
      "#498 Loss: 0.24852041900157928\n",
      "#499 Loss: 0.24852041900157928\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([[ -11.9800,  -10.9900,  -19.1200,  -17.2800,  -33.1300,  -28.4500,\n",
      "          -48.8300,  -40.7700,  -62.4900,  -50.8200,    1.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -12.1500,  -11.3300,  -22.3300,  -19.9500,  -39.8200,  -33.6400,\n",
      "          -56.9000,  -46.7700,  -73.3200,  -58.9600,    1.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -12.5800,  -11.7400,  -26.6700,  -23.3400,  -46.4800,  -38.6900,\n",
      "          -65.9500,  -53.4600,  -84.5300,  -67.2100,    1.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -13.7900,  -12.8200,  -30.5600,  -26.1800,  -52.3000,  -42.9800,\n",
      "          -73.8100,  -59.1900,  -94.4100,  -74.4000,    1.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -15.7300,  -13.8700,  -34.5400,  -28.6500,  -57.4400,  -46.2600,\n",
      "          -80.3700,  -63.4900, -102.9400,  -80.2500,    1.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -58.4100,  -38.6600,  -83.5800,  -57.3300, -110.2400,  -76.8000,\n",
      "         -133.7700,  -96.1300, -171.0500, -124.1500,    0.0000,    1.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -59.6800,  -40.0100,  -84.7500,  -58.7500, -109.9100,  -77.2800,\n",
      "         -130.6500,  -94.6900, -161.8100, -118.7000,    0.0000,    1.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -60.1200,  -40.7300,  -85.1900,  -59.4700, -108.0800,  -76.4300,\n",
      "         -127.5100,  -92.8800, -155.5400, -114.7600,    0.0000,    1.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -60.4100,  -41.2000,  -84.7300,  -59.3100, -106.4600,  -75.3900,\n",
      "         -124.6200,  -90.8800, -151.5600, -112.5800,    0.0000,    1.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -60.8700,  -39.8800,  -84.2200,  -57.5800, -105.3200,  -73.3600,\n",
      "         -122.5900,  -88.2800, -146.9900, -109.5200,    0.0000,    1.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -59.4100,  -61.8600,  -91.6500,  -81.0200, -131.0500, -105.5100,\n",
      "         -165.6800, -126.3000, -207.1600, -153.4400,    0.0000,    0.0000,\n",
      "            1.0000,    0.0000],\n",
      "        [ -61.1600,  -62.9200,  -93.5500,  -82.3900, -133.1100, -107.2700,\n",
      "         -168.6200, -128.9400, -210.8500, -157.0100,    0.0000,    0.0000,\n",
      "            1.0000,    0.0000],\n",
      "        [ -61.8000,  -62.9900,  -94.1500,  -82.4500, -133.2400, -107.1500,\n",
      "         -165.3300, -126.5500, -204.6000, -152.8100,    0.0000,    0.0000,\n",
      "            1.0000,    0.0000],\n",
      "        [ -61.9400,  -62.7800,  -94.4600,  -82.3900, -131.5400, -105.6000,\n",
      "         -160.7400, -123.0800, -195.4900, -146.2600,    0.0000,    0.0000,\n",
      "            1.0000,    0.0000],\n",
      "        [ -62.3400,  -61.8700,  -95.0900,  -81.5300, -128.2300, -102.0500,\n",
      "         -156.3100, -119.7100, -187.9500, -140.7900,    0.0000,    0.0000,\n",
      "            1.0000,    0.0000],\n",
      "        [ -44.1000,  -35.0200,  -66.1300,  -50.4300,  -90.2900,  -68.0000,\n",
      "         -101.7400,  -76.5300, -117.6700,  -88.6300,    0.0000,    0.0000,\n",
      "            0.0000,    1.0000],\n",
      "        [ -44.6700,  -35.3900,  -66.6200,  -50.8400,  -84.4100,  -63.9400,\n",
      "          -97.1100,  -73.3500, -111.0600,  -83.9800,    0.0000,    0.0000,\n",
      "            0.0000,    1.0000],\n",
      "        [ -44.5000,  -35.2700,  -66.6000,  -50.9800,  -80.7900,  -61.2400,\n",
      "          -93.3400,  -70.6700, -106.1800,  -80.4500,    0.0000,    0.0000,\n",
      "            0.0000,    1.0000],\n",
      "        [ -44.3400,  -34.9700,  -66.5800,  -50.9300,  -78.0700,  -59.2700,\n",
      "          -89.9500,  -68.1800, -101.4600,  -77.0600,    0.0000,    0.0000,\n",
      "            0.0000,    1.0000],\n",
      "        [ -44.8000,  -35.1700,  -66.2000,  -50.4200,  -76.8000,  -58.2600,\n",
      "          -87.0900,  -65.5500,  -98.1400,  -73.8900,    0.0000,    0.0000,\n",
      "            0.0000,    1.0000],\n",
      "        [ -52.2200,  -41.6700,  -75.7400,  -57.7000, -109.1000,  -83.4200,\n",
      "         -149.6200, -114.9600, -197.3500, -151.9200,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -54.1500,  -42.9100,  -77.4300,  -59.3300, -112.0400,  -86.1600,\n",
      "         -155.0400, -119.5600, -205.7700, -159.3000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -54.7200,  -43.0400,  -78.1900,  -60.0500, -113.3600,  -87.5000,\n",
      "         -157.6800, -121.9500, -207.9600, -161.6000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -54.5400,  -42.6100,  -78.6700,  -60.5900, -114.3100,  -88.4400,\n",
      "         -158.3500, -122.7700, -207.1500, -161.3700,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000],\n",
      "        [ -54.4700,  -41.8700,  -79.1500,  -60.5800, -115.7200,  -89.2600,\n",
      "         -159.2300, -122.9500, -200.9800, -157.1800,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000]])\n",
      "Output: \n",
      "tensor([[4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06],\n",
      "        [4.7170e-06]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\andirian98\\lib\\site-packages\\torch\\serialization.py:401: UserWarning: Couldn't retrieve source code for container of type Neural_Network. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(500):  # trains the NN 500 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X, y)\n",
    "NN.saveWeights(NN)\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
